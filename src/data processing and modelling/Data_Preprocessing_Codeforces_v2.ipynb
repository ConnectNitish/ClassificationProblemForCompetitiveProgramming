{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:05.860618Z",
     "start_time": "2019-04-19T04:12:49.341829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.metrics import hamming_loss, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import wordcloud\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:05.871533Z",
     "start_time": "2019-04-19T04:13:05.864023Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_statement(statement):\n",
    "#     x = re.sub('-', ' ', x)\n",
    "    statement = re.sub('$', ' ', statement)\n",
    "    statement = re.sub('[^A-Za-z]+', ' ', statement)\n",
    "    statement = re.sub('[,|.|?|\\n]|\\t', '', statement)\n",
    "    statement = re.sub('n\\'t', ' ', statement)\n",
    "    statement = re.sub('submission|submissions|Submission|submission|th ', '', statement)\n",
    "    statement = re.sub('one|two|given|need', '', statement)\n",
    "    \n",
    "    return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:05.941264Z",
     "start_time": "2019-04-19T04:13:05.873645Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process_problem_statement(q_statement):\n",
    "    \n",
    "    q_statement = clean_statement(q_statement)\n",
    "    \n",
    "#     q_statement = re.sub('[^A-Za-z]+', ' ', q_statement)\n",
    "    \n",
    "    tokens = word_tokenize(q_statement)\n",
    "    \n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    \n",
    "    word_list = [i for i in q_statement.lower().split() if i not in stoplist]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "#     word_list = [ps.stem(word) for word in word_list]\n",
    "    \n",
    "    q_statement = ' '.join(word_list)\n",
    "    \n",
    "#     print(q_statement)\n",
    "    \n",
    "    return q_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:05.968820Z",
     "start_time": "2019-04-19T04:13:05.948137Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_problem_solution(solution):\n",
    "    \n",
    "#     solution = clean_statement(solution)\n",
    "    \n",
    "    tokens = word_tokenize(solution)\n",
    "    \n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    \n",
    "    word_list = [i for i in solution.lower().split() if i not in stoplist]\n",
    "    \n",
    "#     ps = PorterStemmer()\n",
    "    \n",
    "#     word_list = [ps.stem(word) for word in word_list]\n",
    "    \n",
    "    solution = ' '.join(word_list)\n",
    "    \n",
    "#     print(q_statement)\n",
    "    \n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:05.994471Z",
     "start_time": "2019-04-19T04:13:05.970336Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_time_taken(time_col):\n",
    "#     print(time_col.split())\n",
    "    return time_col.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:06.016607Z",
     "start_time": "2019-04-19T04:13:05.998036Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tags(all_tags_list,tag_col):\n",
    "    \n",
    "#     print(tag_col)\n",
    "    tags_present = list(re.split(',',tag_col))\n",
    "    \n",
    "    \n",
    "    tags_set = set(tags_present)\n",
    "    tags_diff = tags_set.difference(set(all_tags_list))\n",
    "    \n",
    "    new_set = tags_set.difference(tags_diff)\n",
    "#     print(new_set)\n",
    "    return list(new_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:06.069915Z",
     "start_time": "2019-04-19T04:13:06.018059Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_distinct_tags(tags_col):\n",
    "    \n",
    "    tags_list = []\n",
    "    \n",
    "    t_sets = set(tags_list)\n",
    "    \n",
    "    for row in tags_col:\n",
    "#         print(row)\n",
    "        t_list = re.split(',',row)\n",
    "#         print(t_list)\n",
    "        t_sets = t_sets.union(set(t_list))\n",
    "#         print(t_sets)\n",
    "    tags_list = list(t_sets)\n",
    "    \n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    \n",
    "    word_list = [i for i in tags_list if i not in stoplist]\n",
    "    \n",
    "    return tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:06.099127Z",
     "start_time": "2019-04-19T04:13:06.075880Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# tag list obtained from the dataset\n",
    "# global tags_list\n",
    "\n",
    "tags_list = ['dsu', 'trees', 'chinese remainder theorem', 'sortings', 'games', 'implementation', 'bitmasks',\n",
    "              '*special', 'hashing', 'geometry', 'two pointers', 'combinatorics', 'flows', 'strings',\n",
    "              'probabilities', 'data structures', 'ternary search', 'greedy', 'math', 'matrices',\n",
    "              'divide and conquer', 'dfs and similar', 'constructive algorithms', 'brute force', 'dp',\n",
    "              '2-sat', 'graph matchings', 'binary search', 'number theory', 'graphs', 'fft', 'shortest paths',\n",
    "              'schedules', 'meet-in-the-middle', 'string suffix structures', 'expression parsing']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:06.123474Z",
     "start_time": "2019-04-19T04:13:06.101518Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    \n",
    "    df = pd.read_csv(\"codeforces_question_v4.csv\")\n",
    "    df = df.drop(['id','name','author'],axis = 1)\n",
    "    df = df[df.solution != \"no code found\"]\n",
    "    \n",
    "    global distinct_tags\n",
    "    \n",
    "    distinct_tags = get_all_distinct_tags(df[\"tags\"])\n",
    "    \n",
    "    df[\"problem statement\"] = [process_problem_statement(x) for x in df[\"problem statement\"]]\n",
    "    df[\"solution\"] = [process_problem_solution(x) for x in df[\"solution\"]]\n",
    "    df[\"time_taken\"] = [process_time_taken(x) for x in df[\"time_taken\"]]\n",
    "    \n",
    "    X = copy.deepcopy(df[\"solution\"])\n",
    "    Y = [process_tags(distinct_tags,x) for x in df[\"tags\"]]\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y = mlb.fit_transform(Y)\n",
    "    \n",
    "    \n",
    "    return X, Y, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T19:31:11.971520Z",
     "start_time": "2019-04-08T19:31:04.154403Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Train data\n",
      "hamming_loss:  0.021008136282735825\n",
      "recall_score:  0.6974502866902526\n",
      "precision_score:  0.9674742175553301\n",
      "f1_score:  0.7951027800246083\n",
      "roc_auc_score:  0.8451667439020218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-65f9019c772a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1_score: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roc_auc_score: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"confusion_matrix: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "#validation_fraction = 0.2,early_stopping = True,learning_rate = 'adaptive',eta0 = 0.001,verbose = 2\n",
    "\n",
    "global distinct_tags\n",
    "\n",
    "X,Y, mlb = data_preprocessing()\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range = (1,2),binary = True)),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2',sublinear_tf = True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(penalty=\"l2\",loss=\"squared_hinge\",tol=1\n",
    "            ,random_state=0, max_iter=1000,C = 0.5)))])\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "predicted = classifier.predict(X_train)\n",
    "y_labels_predicted = mlb.inverse_transform(predicted)\n",
    "y_labels_actual = mlb.inverse_transform(Y_train)\n",
    "\n",
    "print(\"On Train data\")\n",
    "print(\"hamming_loss: \",hamming_loss(Y_train,predicted))\n",
    "print(\"recall_score: \",recall_score(Y_train,predicted,average = 'weighted'))\n",
    "print(\"precision_score: \",precision_score(Y_train,predicted,average = 'weighted'))\n",
    "print(\"f1_score: \",f1_score(Y_train,predicted,average = 'weighted'))\n",
    "print(\"roc_auc_score: \",roc_auc_score(Y_train,predicted,average = 'weighted'))\n",
    "print(\"confusion_matrix: \",confusion_matrix(Y_train,predicted))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# print(\"Actual vs Predicted\")\n",
    "\n",
    "# for item, labels in zip(y_labels_actual, y_labels_predicted):\n",
    "#         print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "\n",
    "print(\"On Validation data\")\n",
    "predicted = classifier.predict(X_validation)\n",
    "y_labels_predicted = mlb.inverse_transform(predicted)\n",
    "y_labels_actual = mlb.inverse_transform(Y_validation)\n",
    "print(predicted)\n",
    "print(\"hamming_loss: \",hamming_loss(Y_validation,predicted))\n",
    "print(\"recall_score: \",recall_score(Y_validation,predicted,average = 'weighted'))\n",
    "print(\"precision_score: \",precision_score(Y_validation,predicted,average = 'weighted'))\n",
    "print(\"f1_score: \",f1_score(Y_validation,predicted,average = 'weighted'))\n",
    "# print(\"roc_auc_score: \",roc_auc_score(predicted,predicted,average = 'weighted'))\n",
    "print(\"confusion_matrix: \",confusion_matrix(Y_validation,predicted))\n",
    "print()\n",
    "print()\n",
    "# print(\"Actual vs Predicted\")\n",
    "\n",
    "# for item, labels in zip(y_labels_actual, y_labels_predicted):\n",
    "#         print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# classifier2 = Pipeline([\n",
    "#     ('vectorizer', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', LinearSVC(penalty=\"l2\",loss=\"squared_hinge\",dual=True,tol=0.0000001, C=1.0, \n",
    "#             multi_class=\"ovr\",random_state=0, max_iter=10000))])\n",
    "\n",
    "# print(X_train.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# actual_y = []\n",
    "# predicted_list = []\n",
    "\n",
    "# for index in range(len(distinct_tags)-1):\n",
    "    \n",
    "#     print('Processing tag: {}'.format(distinct_tags[index]))\n",
    "#     classifier2.fit(X_train, Y[:,index])\n",
    "#     predicted = classifier2.predict(X_test)\n",
    "    \n",
    "# #     print(predicted)\n",
    "# #     print(\"##########################################################\")\n",
    "# #     print(target_names.iloc[:,index])\n",
    "#     #     print(mlb.fit_transform(target_names)[:,index])\n",
    "    \n",
    "# #     actual_y.append(mlb.fit_transform(target_names)[:,index])\n",
    "# #     predicted_list.append(predicted)\n",
    "    \n",
    "#     print('Test accuracy is {}'.format(accuracy_score(mlb.fit_transform(target_names)[:,index], predicted)))\n",
    "#     print('Test recall_score is {}'.format(recall_score(mlb.fit_transform(target_names)[:,index], predicted)))\n",
    "#     print('Test precision_score is {}'.format(precision_score(mlb.fit_transform(target_names)[:,index], predicted)))\n",
    "#     print('Test f1_score is {}'.format(f1_score(mlb.fit_transform(target_names)[:,index], predicted)))\n",
    "    \n",
    "#     print()\n",
    "    \n",
    "#     y_true = mlb.fit_transform(target_names)[:,index]\n",
    "#     y_probas = predicted\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_probas, pos_label=0)\n",
    "\n",
    "# #     print(\"$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "# #     print(fpr, tpr, thresholds)\n",
    "# #     print(\"$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "    \n",
    "#     # Print ROC curve\n",
    "#     plt.plot(fpr,tpr)\n",
    "#     plt.show() \n",
    "\n",
    "#     # Print AUC\n",
    "#     auc = np.trapz(tpr,fpr)\n",
    "#     print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:16.328059Z",
     "start_time": "2019-04-19T04:13:16.322850Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_tag_frequency(Y):\n",
    "    \n",
    "    freq = [0]*Y.shape[1]\n",
    "    \n",
    "    for col in range(Y.shape[1]):\n",
    "        \n",
    "        for row in list(Y[:,col]):\n",
    "            if row == 1:\n",
    "                \n",
    "                freq[col] += 1\n",
    "        \n",
    "    return np.array(freq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:22.396339Z",
     "start_time": "2019-04-19T04:13:22.392401Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_class_imbalance(y_predicted,tag_freq):\n",
    "    \n",
    "    for row_index in range(y_predicted.shape[0]):\n",
    "        \n",
    "        for col_index in range(y_predicted.shape[1]):\n",
    "            \n",
    "            y_predicted[row_index,col_index] /= tag_freq[col_index]\n",
    "    \n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T04:13:24.848539Z",
     "start_time": "2019-04-19T04:13:24.503970Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'codeforces_question_v4.csv' does not exist: b'codeforces_question_v4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1a88abeda820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b4a903b6b954>\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"codeforces_question_v4.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolution\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"no code found\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/prakashjha/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/prakashjha/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/prakashjha/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/prakashjha/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/prakashjha/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'codeforces_question_v4.csv' does not exist: b'codeforces_question_v4.csv'"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = data_preprocessing()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train)\n",
    "\n",
    "tag_freq = generate_tag_frequency(Y)\n",
    "\n",
    "n_most_common_words = 8000\n",
    "max_len = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words, filters=';', lower=False)\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "sequences = tokenizer.texts_to_sequences(X_train.values)\n",
    "# print(sequences)\n",
    "word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# print(X)\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "\n",
    "epochs = 2\n",
    "emb_dim = 250\n",
    "batch_size = 100\n",
    "\n",
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
    "# model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.3))\n",
    "# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(36, activation='sigmoid'))\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss=binary_crossentropy, metrics=['binary_accuracy'],optimizer=sgd)\n",
    "\n",
    "print(model.summary())\n",
    "callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.01)]\n",
    "history = model.fit(X_train, y_train[:,0], epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks)\n",
    "\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "accr = model.evaluate(X_test,y_test[:,0])\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "print(y_predicted)\n",
    "\n",
    "print('Test accuracy is {}'.format(accuracy_score(y_test[:,0], y_predicted)))\n",
    "print('Test recall_score is {}'.format(recall_score(y_test[:,0], y_predicted)))\n",
    "print('Test precision_score is {}'.format(precision_score(y_test[:,0], y_predicted)))\n",
    "print('Test f1_score is {}'.format(f1_score(y_test[:,0], y_predicted)))\n",
    "\n",
    "\n",
    "# print(y_predicted)\n",
    "\n",
    "# y_predicted = handle_class_imbalance(y_predicted,tag_freq)\n",
    "\n",
    "# print(y_predicted)\n",
    "\n",
    "# threshold = 0.03\n",
    "\n",
    "# m = []\n",
    "# for row in y_predicted:\n",
    "# #     m.append(softmax(row))\n",
    "#     m.append(row)\n",
    "    \n",
    "# final_prediction = []\n",
    "\n",
    "# print(m)\n",
    "\n",
    "# for row in m:\n",
    "#     temp = []\n",
    "#     val = np.sort(row)[-3]\n",
    "#     for item in row:\n",
    "#         if item < val:\n",
    "#             temp.append(0)\n",
    "#         else:\n",
    "#             temp.append(1)\n",
    "#     final_prediction.append(temp)\n",
    "    \n",
    "# print(final_prediction)    \n",
    "# print(y_test.shape)\n",
    "# print(np.array(final_prediction).shape)\n",
    "# print()\n",
    "# print(\"hamming_loss: \",hamming_loss(np.array(final_prediction),y_test))\n",
    "# print()\n",
    "\n",
    "# all_labels = mlb.inverse_transform(np.array(final_prediction))\n",
    "# y_labels = mlb.inverse_transform(y_test)\n",
    "\n",
    "# for item, labels in zip(y_labels, all_labels):\n",
    "#         print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "\n",
    "# acc = history.history['binary_accuracy']\n",
    "# val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T13:14:12.596173Z",
     "start_time": "2019-04-08T13:13:34.062Z"
    }
   },
   "outputs": [],
   "source": [
    "l = np.array([[1,2,3,4],[2,4,6,7],[45,67,99,2]])\n",
    "print(l)\n",
    "m = []\n",
    "for row in l:\n",
    "    print(row)\n",
    "    m.append(softmax(row))\n",
    "    \n",
    "m = np.array(m)\n",
    "print(m)\n",
    "\n",
    "k = [3,2,4,5,-1,-5,6]\n",
    "\n",
    "np.sort(k)[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T13:13:54.144624Z",
     "start_time": "2019-04-08T13:13:54.138460Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def comments():\n",
    "    \n",
    "    #     global tags_list\n",
    "    #     print(set(distinct_tags).difference(set(tags_list)))\n",
    "\n",
    "\n",
    "    #     print(df[\"tags\"])  \n",
    "    #     print(df[df['difficulty'] == ''])\n",
    "\n",
    "    #     np.where(df.applymap(lambda x: x == ''))\n",
    "\n",
    "    #     nan_rows = df[df['difficulty'].isna()]\n",
    "    #     print(nan_rows)\n",
    "    #     print(df[\"difficulty\"].describe())\n",
    "    #     print(df[\"solution\"].describe())\n",
    "\n",
    "    #     print(df[\"time_taken\"])\n",
    "\n",
    "\n",
    "    #     one_hot = pd.get_dummies(df['tags'])\n",
    "    #     # Drop column B as it is now encoded\n",
    "    #     df = df.drop('tags',axis = 1)\n",
    "    #     # Join the encoded df\n",
    "    #     df = df.join(one_hot)\n",
    "    #     print(df)\n",
    "\n",
    "\n",
    "    #     print(distinct_tags)\n",
    "    #     print(df.describe())\n",
    "    #     print(df[\"problem statement\"])\n",
    "    #     print(df[\"tags\"])\n",
    "\n",
    "    #     cloud = wordcloud.WordCloud(background_color='black', max_font_size=60, relative_scaling=.5).generate(' '.join(df[\"solution\"]))\n",
    "    #     plt.figure(figsize=(20,10))\n",
    "    #     plt.axis('off')\n",
    "    #     plt.imshow(cloud);\n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    \n",
    "    #OneVsRestClassifier(MultinomialNB())\n",
    "    #OneVsRestClassifier(LinearSVC())\n",
    "    #OneVsRestClassifier(LogisticRegression(solver='sag'))\n",
    "\n",
    "    # classifier.fit(X_train, Y)\n",
    "    # predicted = classifier.predict(X_test)\n",
    "    # all_labels = mlb.inverse_transform(predicted)\n",
    "    # predicted = copy.deepcopy(all_labels)\n",
    "    # print(list(map(list, all_labels)))\n",
    "    # all_labels = list(map(list, all_labels))\n",
    "    # print(all_labels)\n",
    "    # all_labels = all_labels.append(distinct_tags)\n",
    "    # target_names.append(distinct_tags)\n",
    "\n",
    "    # print(target_names)\n",
    "    # print(all_labels)\n",
    "    \n",
    "    # print(\"Accuracy: \",accuracy_score(mlb.fit_transform(target_names),mlb.fit_transform(all_labels)))\n",
    "    # print()\n",
    "    # for item, labels in zip(target_names, predicted):\n",
    "    #     print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "    \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
